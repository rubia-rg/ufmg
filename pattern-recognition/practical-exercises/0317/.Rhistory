metric="ROC",
trControl=ctrl)
svm.tune
library('plot3D')
library('MASS')
rm(list=ls())
########################################
# Funções #
fnormal2var <- function(x,y,mx,my,sx,sy,cr){
(1/(2*pi*sx*sy*sqrt(1-cr*cr)))*
exp((-0.5*(1-cr*cr))*((((x-mx)^2)/(sx^2))+
(((y-my)^2)/(sy^2))-
(2*cr*((x-mx)*(y-my))/sy*sx)))}
########################################
# Parâmetros iniciais #
N <- 100
minseq <- 0
maxseq <- 6
########################################
# Gerando dados amostrados das distribuições m1=(2,2)', m2=(4,4)' #
c1 <- matrix(rnorm(N*2),ncol=2)*0.5+c(2,2)
c2 <- matrix(rnorm(N*2),ncol=2)*0.5+c(4,4)
plot(c1[,1], c1[,2], col='red', type='p', xlim=c(minseq,maxseq),
ylim=c(minseq,maxseq),xlab='x',ylab='y')
par(new=T)
plot(c2[,1], c2[,2], col='blue', type='p', xlim=c(minseq,maxseq),
ylim=c(minseq,maxseq),xlab='',ylab='',
sub='Figura 1: Dados amostrados das distribuições normais')
########################################
# Estimando parâmetros das densidades #
# Médias
m11 <- mean(c1[,1]) # C1
m12 <- mean(c1[,2]) # C1
m21 <- mean(c2[,1]) # C2
m22 <- mean(c2[,2]) # C2
# Desvios pradão
s11 <- sd(c1[,1]) # C1
s12 <- sd(c1[,2]) # C1
s21 <- sd(c2[,1]) # C2
s22 <- sd(c2[,2]) # C2
# Coeficientes de correlação
cr1 <- cor(c1[,1],c1[,2])
cr2 <- cor(c2[,1],c2[,2])
########################################
# Estimando a densidade de C1 e C2 #
seqi <- seq(minseq,maxseq,0.1)
seqj <- seq(minseq,maxseq,0.1)
M1 <- matrix(0,nrow=length(seqi),ncol=length(seqj))
M2 <- matrix(0,nrow=length(seqi),ncol=length(seqj))
ci <- 0
for (i in seqi)
{
cj <- 0
ci<- ci + 1
for (j in seqj)
{
cj <- cj + 1
M1[ci,cj]<-fnormal2var(i,j,m11,m12,s11,s12,cr1)
M2[ci,cj]<-fnormal2var(i,j,m21,m22,s21,s22,cr2)
}
}
########################################
# Gráficos das densidades de probabilidade #
persp3D(seqi,seqj,M1,clim=c(0,0.8),colkey=T,
sub='Figura 2: Densidades de probabilidade das distribuições')
persp3D(seqi,seqj,M2,clim=c(0,0.8),add=T,colkey=T,
sub='')
########################################
# Curvas de nível da densidade de probabilidade #
contour2D(M1,seqi,seqj)
par(new=T)
contour2D(M2, seqi,seqj,
sub='Figura 3: Curvas de nível das densidades de probabilidade')
########################################
# Superfície de separação #
classx<-0.7*(M1 >= M2)
# Superfície de separação - 3D #
persp3D(seqi,seqj,M1,clim=c(0,0.8),colkey=T,
sub='Figura 4: Superfície de separação - 3D')
persp3D(seqi,seqj,M2,clim=c(0,0.8),add=T,colkey=T,
sub='')
persp3D(seqi,seqj,classx,clim=c(1.5,0), zlim=c(0,2),add=T,colkey=F,
sub='')
library('plot3D')
library('MASS')
rm(list=ls())
########################################
# Funções #
fnormal2var <- function(x,y,mx,my,sx,sy,cr){
(1/(2*pi*sx*sy*sqrt(1-cr*cr)))*
exp((-0.5*(1-cr*cr))*((((x-mx)^2)/(sx^2))+
(((y-my)^2)/(sy^2))-
(2*cr*((x-mx)*(y-my))/sy*sx)))}
########################################
# Parâmetros iniciais #
N <- 100
minseq <- 0
maxseq <- 6
########################################
# Gerando dados amostrados das distribuições m1=(2,2)', m2=(4,4)' #
c1 <- matrix(rnorm(N*2),ncol=2)*0.5+c(2,2)
c2 <- matrix(rnorm(N*2),ncol=2)*0.5+c(4,4)
plot(c1[,1], c1[,2], col='red', type='p', xlim=c(minseq,maxseq),
ylim=c(minseq,maxseq),xlab='x',ylab='y')
par(new=T)
plot(c2[,1], c2[,2], col='blue', type='p', xlim=c(minseq,maxseq),
ylim=c(minseq,maxseq),xlab='',ylab='',
sub='Figura 1: Dados amostrados das distribuições normais')
library('plot3D')
library('MASS')
rm(list=ls())
###########################
# Funções #
# Densidade de probabilidade multivariada #
pdfnvar <- function(x,m,k,V){
((1/(sqrt(((2*pi)^k)*det(V))))
*exp(-0.5*(t(x-m) %*% solve(V) %*% (x-m))))}
# Classificação Bayesiana #
class <- function(pxc1,pxc2,pc1,pc2){ifelse(((pxc1/pxc2) >= (pc2/pc1)), 1, 2)}
###########################
# Parâmetros #
nc1 <- 100
nc2 <- 50
rep <- 30
###########################
# Dataset Iris #
data(iris)
X <- as.matrix(iris[,1:4])
Y <- as.matrix(iris[,5])
Y1 <- matrix(1, nrow=nc1,ncol=1)
Y2 <- matrix(2, nrow=nc2,ncol=1)
Y <- rbind(Y1,Y2)
###########################
# Amostrar dados #
index <- sample(2, nrow(iris), replace=TRUE, prob=c(0.70,0.30))
###########################
# Conjunto de treinamento #
training <- X[index==1,1:4]
trainingLabels <- Y[index==1]
###########################
# Conjunto de teste #
test <- X[index==2, 1:4]
testLabels <- Y[index==2]
###########################
# Índices das classes c1, c2 #
i1 <- which(trainingLabels==1)
i2 <- which(trainingLabels==2)
###########################
# Probabilidades a priori #
pc1 <- nc1/(nc1+nc2)
pc2 <- nc2/(nc1+nc2)
pairs(Species~., data=iris, main='Dataset Iris', col=iris$Species)
pairs(Species~., data=iris, main='Dataset Iris', col=iris$Species)
###########################
# Verossimilhança e classificação: conjunto de treinamento #
trainingY <- c()
ntr <- dim(training)[1]
for (i in 1:ntr)
{
pxc1 <- pdfnvar(training[i,1:4],colMeans(training[i1,1:4]),
(dim(training)[2]),cov(training[i1,1:4]))
pxc2 <- pdfnvar(training[i,1:4],colMeans(training[i2,1:4]),
(dim(training)[2]),cov(training[i2,1:4]))
trainingY[i] <- ifelse(((pxc1/pxc2) >= (pc2/pc1)), 1, 2)
}
# Matriz de confusão #
trainingCM <- table(trainingY,trainingLabels)
trainingCM
# Acertos #
paste(formatC((((trainingCM[1,1]+trainingCM[2,2])/dim(training)[1])*100),digits=4), "%", sep="")
###########################
# Verossimilhança e classificação: conjunto de teste #
testY <- c()
nte <- dim(test)[1]
for (i in 1:nte)
{
pxc1 <- pdfnvar(test[i,1:4],colMeans(training[i1,1:4]),
(dim(training)[2]),cov(training[i1,1:4]))
pxc2 <- pdfnvar(test[i,1:4],colMeans(training[i2,1:4]),
(dim(training)[2]),cov(training[i2,1:4]))
testY[i] <- ifelse(((pxc1/pxc2) >= (pc2/pc1)), 1, 2)
}
# Matriz de confusão #
testCM <- table(testY,testLabels)
testCM
# Acertos #
paste(formatC((((testCM[1,1]+testCM[2,2])/dim(test)[1])*100),digits=4), "%", sep="")
###########################
# Classificação (para n repetições) #
acc <- c() # acertos por treino
for(j in 1:rep){
###########################
# Misturar dados #
index <- sample(2, nrow(iris), replace=TRUE, prob=c(0.70,0.30))
###########################
# Conjunto de treinamento #
training <- X[index==1,1:4]
trainingLabels <- Y[index==1]
###########################
# Conjunto de teste #
test <- X[index==2, 1:4]
testLabels <- Y[index==2]
###########################
# Classificação #
# Índices das classes c1, c2 #
i1 <- which(trainingLabels==1)
i2 <- which(trainingLabels==2)
# Probabilidade a priori #
pc1 <- nc1/(nc1+nc2)
pc2 <- nc2/(nc1+nc2)
# Verossimilhança e classificação: Conjunto de teste #
testY <- c()
nte <- dim(test)[1]
for (i in 1:nte)
{
pxc1 <- pdfnvar(test[i,1:4],colMeans(training[i1,1:4]),
(dim(training)[2]),cov(training[i1,1:4]))
pxc2 <- pdfnvar(test[i,1:4],colMeans(training[i2,1:4]),
(dim(training)[2]),cov(training[i2,1:4]))
testY[i] <- ifelse(((pxc1/pxc2) >= (pc2/pc1)), 1, 2)
}
# Matriz de confusão #
testCM <- table(testY,testLabels)
# Acertos #
acc[j] <- (testCM[1,1]+testCM[2,2])/(dim(test)[1])
}
###########################
# Média e desvio padrão dos acertos #
paste(formatC((mean(acc)*100),digits=4), "%", sep="")
paste(formatC((sd(acc)*100),digits=3), "%", sep="")
rm(list=ls())
library('MASS')
distance <- function(xt, centers){
distMatrix <- matrix(NA, nrow=dim(xt)[1], ncol=dim(centers)[1])
for(i in 1:nrow(centers)) {
distMatrix[,i] <- sqrt(rowSums(t(t(xt)-centers[i,])^2))
}
distMatrix
}
mykmeans <- function(x, k, maxIter) {
clusterOld <- c()
centerOld <- c()
centers <- x[sample(nrow(x), k),]
flag <- FALSE
i <- 0
while(i <= maxIter && flag==FALSE) {
i <- i + 1
if(i > 1) {
clusterOld <- clusters
centerOld <- centers
}
distsToCenters <- distance(x, centers)
clusters <- apply(distsToCenters, 1, which.min)
centers <- apply(x, 2, tapply, clusters, mean)
flag <- identical(clusters,clusterOld)
}
list(clusters=clusters, centers=centers)
}
##################################################################
k <- c(2, 4, 8)
sd2 <- (c(0.3, 0.5, 0.7)^2)
for(j in 1:length(sd))
{
for(i in 1:length(k))
{
N <- 100
maxIter <- 100
cores <- rainbow(k[i])
S <- matrix(c(sd2[j],0,0,sd2[j]),byrow=T,ncol=2)
g1 <- mvrnorm(N,mu=c(2,2), Sigma=S)
g2 <- mvrnorm(N,mu=c(2,4), Sigma=S)
g3 <- mvrnorm(N,mu=c(4,2), Sigma=S)
g4 <- mvrnorm(N,mu=c(4,4), Sigma=S)
samples <- rbind(g1,g2,g3,g4)
b <- mykmeans(samples, k[i], maxIter)
for(l in 1:(k[i]-1))
{
plot(samples[b$clusters==l,1],samples[b$clusters==l,2],type='p',col=cores[l],xlab='',ylab='',xlim=c(0,6),ylim=c(0,6))
par(new=T)
}
plot(samples[b$clusters==k[i],1],samples[b$clusters==k[i],2],type='p',col=cores[k[i]],xlab='',ylab='',xlim=c(0,6),ylim=c(0,6))
}
}
rm(list=ls())
library('MASS')
library('mlbench')
library('mclust')
###########################
# Dataset BreastCancer #
data(BreastCancer)
X<- data.matrix(BreastCancer[,2:10])
X[is.na(X)] <- 0
Y <- as.numeric(BreastCancer$Class)
###########################
# Auxiliares #
tp <- c()
fp <- c()
fn <- c()
prec <- c()
rec <- c()
f1 <- c()
error <- c()
mse <- c()
sde <- c()
for(j in 1:10){
###########################
# Amostrar dados #
index <- sample(2, nrow(BreastCancer), replace=TRUE, prob=c(0.70,0.30))
###########################
# Conjunto de treinamento #
training <- X[which(index==1),]
trainingLabels <- as.matrix(Y[which(index==1)])
###########################
# Conjunto de teste #
test <- X[which(index==2),]
testLabels <- as.matrix(Y[which(index==2)])
###########################
# Probabilidades a priori #
pc1 <- length(Y[which(Y==1)])/(length(Y))
pc2 <- length(Y[which(Y==2)])/(length(Y))
###########################
# Treinamento #
mod1 = densityMclust(training[which(trainingLabels==1),])
mod2 = densityMclust(training[which(trainingLabels==2),])
###########################
# Teste #
pxc1 <- dens(modelName=mod1$modelName, data = test, parameters = mod1$parameters)
pxc2 <- dens(modelName=mod2$modelName, data = test, parameters = mod2$parameters)
###########################
# Classificação #
Ntest <- dim(test)[1]
testY <- c()
for(i in 1:Ntest)
{
testY[i] <- ifelse(pxc1[i]/pxc2[i] >= pc2/pc1, 1, 2)
error[i] <- (testY[i]-testLabels[i])^2
}
# MSE e SD #
mse[j] <- mean(error)
sde[j] <- sd(error)
# Matriz de confusao #
testCM <- table(testY,testLabels)
# Precision, recall, F1 #
tp[j] <- sum((testY==1) & (testLabels==1)) # True positives
fp[j] <- sum((testY==1) & (testLabels==2)) # False positives
fn[j] <- sum((testY==2) & (testLabels==1)) # False negatives
prec[j] <- tp[j]/(tp[j] + fp[j]) # Precision
rec[j] <- tp[j]/(tp[j] + fn[j]) # Recall
f1[j] <- 2*prec[j]*rec[j]/(prec[j]+rec[j]) # F1 Score
}
mean(mse) # MSE
mean(sde) # SD
###########################
# Dataset BreastCancer #
data(BreastCancer)
X<- data.matrix(BreastCancer[,2:10])
X[is.na(X)] <- 0
Y <- as.numeric(BreastCancer$Class)
###########################
# Auxiliares #
tp <- c()
fp <- c()
fn <- c()
prec <- c()
rec <- c()
f1 <- c()
error <- c()
mse <- c()
sde <- c()
for(j in 1:10){
###########################
# Amostrar dados #
index <- sample(2, nrow(BreastCancer), replace=TRUE, prob=c(0.70,0.30))
###########################
# Conjunto de treinamento #
training <- X[which(index==1),]
trainingLabels <- as.matrix(Y[which(index==1)])
###########################
# Conjunto de teste #
test <- X[which(index==2),]
testLabels <- as.matrix(Y[which(index==2)])
###########################
# Probabilidades a priori #
pc1 <- length(Y[which(Y==1)])/(length(Y))
pc2 <- length(Y[which(Y==2)])/(length(Y))
###########################
# Treinamento #
mod1 = densityMclust(training[which(trainingLabels==1),])
mod2 = densityMclust(training[which(trainingLabels==2),])
###########################
# Teste #
pxc1 <- dens(modelName=mod1$modelName, data = test, parameters = mod1$parameters)
pxc2 <- dens(modelName=mod2$modelName, data = test, parameters = mod2$parameters)
###########################
# Classificação #
Ntest <- dim(test)[1]
testY <- c()
for(i in 1:Ntest)
{
testY[i] <- ifelse(pxc1[i]/pxc2[i] >= pc2/pc1, 1, 2)
error[i] <- (testY[i]-testLabels[i])^2
}
# MSE e SD #
mse[j] <- mean(error)
sde[j] <- sd(error)
# Matriz de confusao #
testCM <- table(testY,testLabels)
# Precision, recall, F1 #
tp[j] <- sum((testY==1) & (testLabels==1)) # True positives
fp[j] <- sum((testY==1) & (testLabels==2)) # False positives
fn[j] <- sum((testY==2) & (testLabels==1)) # False negatives
prec[j] <- tp[j]/(tp[j] + fp[j]) # Precision
rec[j] <- tp[j]/(tp[j] + fn[j]) # Recall
f1[j] <- 2*prec[j]*rec[j]/(prec[j]+rec[j]) # F1 Score
}
mean(mse) # MSE
mean(sde) # SD
rm(list=ls())
library('MASS')
library('mlbench')
library('flexclust')
mykde <- function(x, X, n, N, h)
{
return((1/(N*(sqrt(2*pi)*h)^n))*
sum(exp(-((dist2(t(x),X)^2)/((2*h)^2)))))
}
###########################
# Auxiliares #
tp <- c()
fp <- c()
fn <- c()
prec <- c()
rec <- c()
f1 <- c()
error <- c()
mse <- c()
sde <- c()
###########################
# Parâmetros #
niter <- 10 # Número de iterações
ptrain <- 0.7 # % conjunto de treino
ptest <- 1 - ptrain # % conjunto de teste
h <- 0.25
###########################
# Dataset BreastCancer #
data(BreastCancer)
X <- data.matrix(BreastCancer[,2:10])
X[is.na(X)] <- 0
Y <- as.numeric(BreastCancer$Class)
N <- dim(X)[1]
n <- dim(X)[2]
for(j in 1:niter){
###########################
# Amostrar dados #
index <- sample(2, nrow(BreastCancer), replace=TRUE, prob=c(ptrain,ptest))
###########################
# Conjunto de treinamento #
training <- X[which(index==1),]
trainingLabels <- as.matrix(Y[which(index==1)])
###########################
# Conjunto de teste #
test <- X[which(index==2),]
testLabels <- as.matrix(Y[which(index==2)])
###########################
# Probabilidades a priori #
pc1 <- length(Y[which(Y==1)])/(length(Y))
pc2 <- length(Y[which(Y==2)])/(length(Y))
###########################
# Treinamento, teste e classificação #
Ntest <- dim(test)[1]
Ntrain <- dim(training)[1]
trc1 <- training[which(trainingLabels==1),]
trc2 <- training[which(trainingLabels==2),]
testY <- c()
pxc1 <- c()
pxc2 <- c()
p11 <- c()
p12 <- c()
p21 <- c()
p22 <- c()
Nc1 <-  dim(trc1)[1]
Nc2 <-  dim(trc2)[1]
for(i in 1:Nc1)
{
p11[i] <- mykde(trc1[i,],trc1,n,N,h)
p12[i] <- mykde(trc1[i,],trc2,n,N,h)
}
for(i in 1:Nc2)
{
p21[i] <- mykde(trc2[i,],trc1,n,N,h)
p22[i] <- mykde(trc2[i,],trc2,n,N,h)
}
for(i in 1:Ntest)
{
pxc1[i] <- mykde(test[i,],trc1,n,Ntrain,h)
pxc2[i] <- mykde(test[i,],trc2,n,Ntrain,h)
testY[i] <- ifelse(pxc1[i]/pxc2[i] >= pc2/pc1, 1, 2)
error[i] <- (testY[i]-testLabels[i])^2
}
# MSE e SD #
mse[j] <- mean(error)
sde[j] <- sd(error)
# Matriz de confusao #
testCM <- table(testY,testLabels)
# Precision, recall, F1 #
tp[j] <- sum((testY==1) & (testLabels==1)) # True positives
fp[j] <- sum((testY==1) & (testLabels==2)) # False positives
fn[j] <- sum((testY==2) & (testLabels==1)) # False negatives
prec[j] <- tp[j]/(tp[j] + fp[j]) # Precision
rec[j] <- tp[j]/(tp[j] + fn[j]) # Recall
f1[j] <- 2*prec[j]*rec[j]/(prec[j]+rec[j]) # F1 Score
}
