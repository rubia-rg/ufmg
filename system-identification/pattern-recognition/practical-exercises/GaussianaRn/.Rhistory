###########################
# Funções #
# Densidade de probabilidade multivariada #
pdfnvar <- function(x,m,k,V){
((1/(sqrt(((2*pi)^k)*det(V))))
*exp(-0.5*(t(x-m) %*% solve(V) %*% (x-m))))}
# Classificação Bayesiana #
class <- function(pxc1,pxc2,pc1,pc2){ifelse(((pxc1/pxc2) >= (pc2/pc1)), 1, 2)}
###########################
# Parâmetros #
nc1 <- 100
nc2 <- 50
###########################
# Dataset Iris #
data(iris)
as.matrix(iris)
X <- as.matrix(iris[,1:4])
Y <- as.matrix(iris[,5])
Y1 <- matrix(1, nrow=nc1,ncol=1)
Y2 <- matrix(2, nrow=nc2,ncol=1)
Y <- rbind(Y1,Y2)
###########################
# Misturar dados #
set.seed(1234)
index <- sample(2, nrow(iris), replace=TRUE, prob=c(0.70,0.30))
###########################
# Conjunto de treinamento #
training <- X[index==1,1:4]
trainingLabels <- Y[index==1]
###########################
# Conjunto de teste #
test <- X[index==2, 1:4]
testLabels <- Y[index==2]
###########################
# Classificação #
# Índices das classes c1, c2 #
i1 <- which(trainingLabels==1)
i2 <- which(trainingLabels==2)
# Probabilidade a priori #
pc1 <- nc1/(nc1+nc2)
pc2 <- nc2/(nc1+nc2)
# Verossimilhança e classificação: Conjunto de treinamento #
ytr <- c()
ntr <- dim(training)[1]
for (i in 1:ntr)
{
pxc1 <- pdfnvar(training[i,1:4],colMeans(training[i1,1:4]),(dim(training)[2]),cov(training[i1,1:4]))
pxc2 <- pdfnvar(training[i,1:4],colMeans(training[i2,1:4]),(dim(training)[2]),cov(training[i2,1:4]))
ytr[i] <- ifelse(((pxc1/pxc2) >= (pc2/pc1)), 1, 2)
}
# Matriz de confusão #
trainingCM <- table(ytr,trainingLabels)
# Verossimilhança e classificação: Conjunto de teste #
yte <- c()
nte <- dim(test)[1]
for (i in 1:nte)
{
pxc1 <- pdfnvar(test[i,1:4],colMeans(training[i1,1:4]),(dim(training)[2]),cov(training[i1,1:4]))
pxc2 <- pdfnvar(test[i,1:4],colMeans(training[i2,1:4]),(dim(training)[2]),cov(training[i2,1:4]))
yte[i] <- ifelse(((pxc1/pxc2) >= (pc2/pc1)), 1, 2)
}
# Matriz de confusão #
testCM <- table(yte,testLabels)
acc <- (testCM[1,1] + testCM[2,2])/dim(test)[1]
###########################
acc
###########################
# Verossimilhança e classificação: Conjunto de treinamento #
ytr <- c()
ntr <- dim(training)[1]
for (i in 1:ntr)
{
pxc1 <- pdfnvar(training[i,1:4],colMeans(training[i1,1:4]),(dim(training)[2]),cov(training[i1,1:4]))
pxc2 <- pdfnvar(training[i,1:4],colMeans(training[i2,1:4]),(dim(training)[2]),cov(training[i2,1:4]))
ytr[i] <- ifelse(((pxc1/pxc2) >= (pc2/pc1)), 1, 2)
}
# Matriz de confusão #
trainingCM <- table(ytr,trainingLabels)
# Acurácia #
acc <- (trainingCM[1,1] + trainingCM[2,2])/dim(training)[1]
acc
###########################
# Verossimilhança e classificação: Conjunto de treinamento #
ytr <- c()
ntr <- dim(training)[1]
for (i in 1:ntr)
{
pxc1 <- pdfnvar(training[i,1:4],colMeans(training[i1,1:4]),(dim(training)[2]),cov(training[i1,1:4]))
pxc2 <- pdfnvar(training[i,1:4],colMeans(training[i2,1:4]),(dim(training)[2]),cov(training[i2,1:4]))
ytr[i] <- ifelse(((pxc1/pxc2) >= (pc2/pc1)), 1, 2)
}
# Matriz de confusão #
trainingCM <- table(ytr,trainingLabels)
# Acurácia #
acctr <- (trainingCM[1,1] + trainingCM[2,2])/dim(training)[1]
acctr
library('plot3D')
library('MASS')
rm(list=ls())
###########################
# Funções #
# Densidade de probabilidade multivariada #
pdfnvar <- function(x,m,k,V){
((1/(sqrt(((2*pi)^k)*det(V))))
*exp(-0.5*(t(x-m) %*% solve(V) %*% (x-m))))}
# Classificação Bayesiana #
class <- function(pxc1,pxc2,pc1,pc2){ifelse(((pxc1/pxc2) >= (pc2/pc1)), 1, 2)}
###########################
# Parâmetros #
nc1 <- 50
nc2 <- 100
###########################
# Dataset Iris #
data(iris)
as.matrix(iris)
X <- as.matrix(iris[,1:4])
Y <- as.matrix(iris[,5])
Y1 <- matrix(1, nrow=nc1,ncol=1)
Y2 <- matrix(2, nrow=nc2,ncol=1)
Y <- rbind(Y1,Y2)
###########################
# Misturar dados #
set.seed(1234)
index <- sample(2, nrow(iris), replace=TRUE, prob=c(0.70,0.30))
###########################
# Conjunto de treinamento #
training <- X[index==1,1:4]
trainingLabels <- Y[index==1]
###########################
# Conjunto de teste #
test <- X[index==2, 1:4]
testLabels <- Y[index==2]
###########################
# Classificação #
# Índices das classes c1, c2 #
i1 <- which(trainingLabels==1)
i2 <- which(trainingLabels==2)
# Probabilidade a priori #
pc1 <- nc1/(nc1+nc2)
pc2 <- nc2/(nc1+nc2)
# Verossimilhança e classificação: Conjunto de teste #
yte <- c()
nte <- dim(test)[1]
for (i in 1:nte)
{
pxc1 <- pdfnvar(test[i,1:4],colMeans(training[i1,1:4]),(dim(training)[2]),cov(training[i1,1:4]))
pxc2 <- pdfnvar(test[i,1:4],colMeans(training[i2,1:4]),(dim(training)[2]),cov(training[i2,1:4]))
yte[i] <- ifelse(((pxc1/pxc2) >= (pc2/pc1)), 1, 2)
}
# Matriz de confusão #
testCM <- table(yte,testLabels)
# Acurácia #
acc <- (testCM[1,1] + testCM[2,2])/dim(test)[1]
###########################
###########################
# Verossimilhança e classificação: Conjunto de treinamento #
ytr <- c()
ntr <- dim(training)[1]
for (i in 1:ntr)
{
pxc1 <- pdfnvar(training[i,1:4],colMeans(training[i1,1:4]),(dim(training)[2]),cov(training[i1,1:4]))
pxc2 <- pdfnvar(training[i,1:4],colMeans(training[i2,1:4]),(dim(training)[2]),cov(training[i2,1:4]))
ytr[i] <- ifelse(((pxc1/pxc2) >= (pc2/pc1)), 1, 2)
}
# Matriz de confusão #
trainingCM <- table(ytr,trainingLabels)
# Acurácia #
acctr <- (trainingCM[1,1] + trainingCM[2,2])/dim(training)[1]
acctr
library('plot3D')
library('MASS')
rm(list=ls())
###########################
# Funções #
# Densidade de probabilidade multivariada #
pdfnvar <- function(x,m,k,V){
((1/(sqrt(((2*pi)^k)*det(V))))
*exp(-0.5*(t(x-m) %*% solve(V) %*% (x-m))))}
# Classificação Bayesiana #
class <- function(pxc1,pxc2,pc1,pc2){ifelse(((pxc1/pxc2) >= (pc2/pc1)), 1, 2)}
###########################
# Parâmetros #
nc1 <- 50
nc2 <- 100
rep <- 30
###########################
# Dataset Iris #
data(iris)
as.matrix(iris)
X <- as.matrix(iris[,1:4])
Y <- as.matrix(iris[,5])
Y1 <- matrix(1, nrow=nc1,ncol=1)
Y2 <- matrix(2, nrow=nc2,ncol=1)
Y <- rbind(Y1,Y2)
for(j in 1:rep){
###########################
# Misturar dados #
set.seed(1234)
index <- sample(2, nrow(iris), replace=TRUE, prob=c(0.70,0.30))
###########################
# Conjunto de treinamento #
training <- X[index==1,1:4]
trainingLabels <- Y[index==1]
###########################
# Conjunto de teste #
test <- X[index==2, 1:4]
testLabels <- Y[index==2]
###########################
# Classificação #
# Índices das classes c1, c2 #
i1 <- which(trainingLabels==1)
i2 <- which(trainingLabels==2)
# Probabilidade a priori #
pc1 <- nc1/(nc1+nc2)
pc2 <- nc2/(nc1+nc2)
# Verossimilhança e classificação: Conjunto de teste #
yte <- c()
nte <- dim(test)[1]
for (i in 1:nte)
{
pxc1 <- pdfnvar(test[i,1:4],colMeans(training[i1,1:4]),(dim(training)[2]),cov(training[i1,1:4]))
pxc2 <- pdfnvar(test[i,1:4],colMeans(training[i2,1:4]),(dim(training)[2]),cov(training[i2,1:4]))
yte[i] <- ifelse(((pxc1/pxc2) >= (pc2/pc1)), 1, 2)
}
# Matriz de confusão #
testCM <- table(yte,testLabels)
# Acurácia #
acc[j] <- (testCM[1,1] + testCM[2,2])/dim(test)[1]
###########################
}
library('plot3D')
library('MASS')
rm(list=ls())
###########################
# Funções #
# Densidade de probabilidade multivariada #
pdfnvar <- function(x,m,k,V){
((1/(sqrt(((2*pi)^k)*det(V))))
*exp(-0.5*(t(x-m) %*% solve(V) %*% (x-m))))}
# Classificação Bayesiana #
class <- function(pxc1,pxc2,pc1,pc2){ifelse(((pxc1/pxc2) >= (pc2/pc1)), 1, 2)}
###########################
# Parâmetros #
nc1 <- 50
nc2 <- 100
rep <- 30
###########################
# Dataset Iris #
data(iris)
as.matrix(iris)
X <- as.matrix(iris[,1:4])
Y <- as.matrix(iris[,5])
Y1 <- matrix(1, nrow=nc1,ncol=1)
Y2 <- matrix(2, nrow=nc2,ncol=1)
Y <- rbind(Y1,Y2)
###########################
# Classificação #
acc <- c()
for(j in 1:rep){
###########################
# Misturar dados #
set.seed(1234)
index <- sample(2, nrow(iris), replace=TRUE, prob=c(0.70,0.30))
###########################
# Conjunto de treinamento #
training <- X[index==1,1:4]
trainingLabels <- Y[index==1]
###########################
# Conjunto de teste #
test <- X[index==2, 1:4]
testLabels <- Y[index==2]
###########################
# Classificação #
# Índices das classes c1, c2 #
i1 <- which(trainingLabels==1)
i2 <- which(trainingLabels==2)
# Probabilidade a priori #
pc1 <- nc1/(nc1+nc2)
pc2 <- nc2/(nc1+nc2)
# Verossimilhança e classificação: Conjunto de teste #
yte <- c()
nte <- dim(test)[1]
for (i in 1:nte)
{
pxc1 <- pdfnvar(test[i,1:4],colMeans(training[i1,1:4]),(dim(training)[2]),cov(training[i1,1:4]))
pxc2 <- pdfnvar(test[i,1:4],colMeans(training[i2,1:4]),(dim(training)[2]),cov(training[i2,1:4]))
yte[i] <- ifelse(((pxc1/pxc2) >= (pc2/pc1)), 1, 2)
}
# Matriz de confusão #
testCM <- table(yte,testLabels)
# Acurácia #
acc[j] <- (testCM[1,1] + testCM[2,2])/dim(test)[1]
###########################
}
acc
mean(acc)
sd(acc)
library('plot3D')
library('MASS')
rm(list=ls())
###########################
# Funções #
# Densidade de probabilidade multivariada #
pdfnvar <- function(x,m,k,V){
((1/(sqrt(((2*pi)^k)*det(V))))
*exp(-0.5*(t(x-m) %*% solve(V) %*% (x-m))))}
# Classificação Bayesiana #
class <- function(pxc1,pxc2,pc1,pc2){ifelse(((pxc1/pxc2) >= (pc2/pc1)), 1, 2)}
###########################
# Parâmetros #
nc1 <- 100
nc2 <- 50
rep <- 30
###########################
# Dataset Iris #
data(iris)
as.matrix(iris)
X <- as.matrix(iris[,1:4])
Y <- as.matrix(iris[,5])
Y1 <- matrix(1, nrow=nc1,ncol=1)
Y2 <- matrix(2, nrow=nc2,ncol=1)
Y <- rbind(Y1,Y2)
###########################
# Classificação #
acc <- c()
for(j in 1:rep){
###########################
# Misturar dados #
set.seed(1234)
index <- sample(2, nrow(iris), replace=TRUE, prob=c(0.70,0.30))
###########################
# Conjunto de treinamento #
training <- X[index==1,1:4]
trainingLabels <- Y[index==1]
###########################
# Conjunto de teste #
test <- X[index==2, 1:4]
testLabels <- Y[index==2]
###########################
# Classificação #
# Índices das classes c1, c2 #
i1 <- which(trainingLabels==1)
i2 <- which(trainingLabels==2)
# Probabilidade a priori #
pc1 <- nc1/(nc1+nc2)
pc2 <- nc2/(nc1+nc2)
# Verossimilhança e classificação: Conjunto de teste #
yte <- c()
nte <- dim(test)[1]
for (i in 1:nte)
{
pxc1 <- pdfnvar(test[i,1:4],colMeans(training[i1,1:4]),(dim(training)[2]),cov(training[i1,1:4]))
pxc2 <- pdfnvar(test[i,1:4],colMeans(training[i2,1:4]),(dim(training)[2]),cov(training[i2,1:4]))
yte[i] <- ifelse(((pxc1/pxc2) >= (pc2/pc1)), 1, 2)
}
# Matriz de confusão #
testCM <- table(yte,testLabels)
# Acurácia #
acc[j] <- (testCM[1,1] + testCM[2,2])/dim(test)[1]
###########################
}
acc
mean(acc)
sd(acc)
library('plot3D')
library('MASS')
rm(list=ls())
###########################
# Funções #
# Densidade de probabilidade multivariada #
pdfnvar <- function(x,m,k,V){
((1/(sqrt(((2*pi)^k)*det(V))))
*exp(-0.5*(t(x-m) %*% solve(V) %*% (x-m))))}
# Classificação Bayesiana #
class <- function(pxc1,pxc2,pc1,pc2){ifelse(((pxc1/pxc2) >= (pc2/pc1)), 1, 2)}
###########################
# Parâmetros #
nc1 <- 100
nc2 <- 50
rep <- 30
###########################
# Dataset Iris #
data(iris)
as.matrix(iris)
X <- as.matrix(iris[,1:4])
Y <- as.matrix(iris[,5])
Y1 <- matrix(1, nrow=nc1,ncol=1)
Y2 <- matrix(2, nrow=nc2,ncol=1)
Y <- rbind(Y1,Y2)
###########################
# Classificação #
acc <- c()
for(j in 1:rep){
###########################
# Misturar dados #
index <- sample(2, nrow(iris), replace=TRUE, prob=c(0.70,0.30))
###########################
# Conjunto de treinamento #
training <- X[index==1,1:4]
trainingLabels <- Y[index==1]
###########################
# Conjunto de teste #
test <- X[index==2, 1:4]
testLabels <- Y[index==2]
###########################
# Classificação #
# Índices das classes c1, c2 #
i1 <- which(trainingLabels==1)
i2 <- which(trainingLabels==2)
# Probabilidade a priori #
pc1 <- nc1/(nc1+nc2)
pc2 <- nc2/(nc1+nc2)
# Verossimilhança e classificação: Conjunto de teste #
yte <- c()
nte <- dim(test)[1]
for (i in 1:nte)
{
pxc1 <- pdfnvar(test[i,1:4],colMeans(training[i1,1:4]),(dim(training)[2]),cov(training[i1,1:4]))
pxc2 <- pdfnvar(test[i,1:4],colMeans(training[i2,1:4]),(dim(training)[2]),cov(training[i2,1:4]))
yte[i] <- ifelse(((pxc1/pxc2) >= (pc2/pc1)), 1, 2)
}
# Matriz de confusão #
testCM <- table(yte,testLabels)
# Acurácia #
acc[j] <- (testCM[1,1] + testCM[2,2])/dim(test)[1]
###########################
}
acc
mean(acc)
sd(acc)
library('plot3D')
library('MASS')
rm(list=ls())
###########################
# Funções #
# Densidade de probabilidade multivariada #
pdfnvar <- function(x,m,k,V){
((1/(sqrt(((2*pi)^k)*det(V))))
*exp(-0.5*(t(x-m) %*% solve(V) %*% (x-m))))}
# Classificação Bayesiana #
class <- function(pxc1,pxc2,pc1,pc2){ifelse(((pxc1/pxc2) >= (pc2/pc1)), 1, 2)}
###########################
# Parâmetros #
nc1 <- 50
nc2 <- 100
rep <- 30
###########################
# Dataset Iris #
data(iris)
as.matrix(iris)
X <- as.matrix(iris[,1:4])
Y <- as.matrix(iris[,5])
Y1 <- matrix(1, nrow=nc1,ncol=1)
Y2 <- matrix(2, nrow=nc2,ncol=1)
Y <- rbind(Y1,Y2)
###########################
# Classificação #
acc <- c()
for(j in 1:rep){
###########################
# Misturar dados #
index <- sample(2, nrow(iris), replace=TRUE, prob=c(0.70,0.30))
###########################
# Conjunto de treinamento #
training <- X[index==1,1:4]
trainingLabels <- Y[index==1]
###########################
# Conjunto de teste #
test <- X[index==2, 1:4]
testLabels <- Y[index==2]
###########################
# Classificação #
# Índices das classes c1, c2 #
i1 <- which(trainingLabels==1)
i2 <- which(trainingLabels==2)
# Probabilidade a priori #
pc1 <- nc1/(nc1+nc2)
pc2 <- nc2/(nc1+nc2)
# Verossimilhança e classificação: Conjunto de teste #
yte <- c()
nte <- dim(test)[1]
for (i in 1:nte)
{
pxc1 <- pdfnvar(test[i,1:4],colMeans(training[i1,1:4]),(dim(training)[2]),cov(training[i1,1:4]))
pxc2 <- pdfnvar(test[i,1:4],colMeans(training[i2,1:4]),(dim(training)[2]),cov(training[i2,1:4]))
yte[i] <- ifelse(((pxc1/pxc2) >= (pc2/pc1)), 1, 2)
}
# Matriz de confusão #
testCM <- table(yte,testLabels)
# Acurácia #
acc[j] <- (testCM[1,1] + testCM[2,2])/dim(test)[1]
###########################
}
acc
mean(acc)
sd(acc)
###########################
# Verossimilhança e classificação: Conjunto de treinamento #
ytr <- c()
ntr <- dim(training)[1]
for (i in 1:ntr)
{
pxc1 <- pdfnvar(training[i,1:4],colMeans(training[i1,1:4]),(dim(training)[2]),cov(training[i1,1:4]))
pxc2 <- pdfnvar(training[i,1:4],colMeans(training[i2,1:4]),(dim(training)[2]),cov(training[i2,1:4]))
ytr[i] <- ifelse(((pxc1/pxc2) >= (pc2/pc1)), 1, 2)
}
# Matriz de confusão #
trainingCM <- table(ytr,trainingLabels)
# Acurácia #
acctr <- (trainingCM[1,1] + trainingCM[2,2])/dim(training)[1]
acctr
iris
